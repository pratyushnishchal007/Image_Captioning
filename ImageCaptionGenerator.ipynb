{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d806389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT MODULES\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e758fcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# LOAD VGG MODEL\n",
    "model=VGG16()\n",
    "# Restructure the model\n",
    "model=Model(inputs=model.inputs,outputs=model.layers[-2].output)\n",
    "# summarize\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d19c6de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be21574a082b49fba948dde4fdddb011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract features from image\n",
    "# key is image id and value is the features\n",
    "features={}\n",
    "directory=os.path.join('Images')\n",
    "for img_name in tqdm(os.listdir(directory)):\n",
    "    # load the image\n",
    "    filepath=directory+'/'+img_name\n",
    "    image=load_img(filepath,target_size=(224,224))\n",
    "    #convert image pixels to numpy array\n",
    "    image=img_to_array(image)\n",
    "    #reshaping the data for the model\n",
    "    image=image.reshape((1,image.shape[0],image.shape[1],image.shape[2]))\n",
    "    #preprocess the image for vgg \n",
    "    image=preprocess_input(image)\n",
    "    #extract features\n",
    "    feature=model.predict(image,verbose=0)\n",
    "    #get image ID\n",
    "    image_id=img_name.split('.')[0]\n",
    "    #store feature\n",
    "    features[image_id]=feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d09cf7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store features in pickle\n",
    "pickle.dump(features,open(os.path.join('features.pkl'),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9181165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load feature from pickle\n",
    "with open(os.path.join('features.pkl'),'rb') as f:\n",
    "    features=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4142ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the captions data\n",
    "with open(os.path.join('captions.txt'),'r') as f:\n",
    "    next(f)\n",
    "    captions_doc=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e33f81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da53f8ceecce41dcbfc7694a2dcce0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create mapping of image to captions\n",
    "mapping={}\n",
    "#process lines\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    #split the lines by comma\n",
    "    tokens=line.split(',')\n",
    "    if len(line)<2:\n",
    "        continue\n",
    "    elif len(line)>2:\n",
    "        image_id=tokens[0].split('.')[0]\n",
    "        caption=\" \".join(tokens[1:])\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id]=[]\n",
    "            #store the caption\n",
    "        mapping[image_id].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5cabbcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71ce34bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def clean(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            # take one caption at a time\n",
    "            caption = captions[i]\n",
    "            # preprocessing steps\n",
    "            # convert to lowercase\n",
    "            caption = caption.lower()\n",
    "            # removing numbers from captions\n",
    "            caption=re.sub(r'[0 - 9]', ' ', caption)\n",
    "            #removing special characters from captions\n",
    "            pattern = r'[' + string.punctuation + ']'\n",
    "            #Remove special characters from the string\n",
    "            caption = re.sub(pattern, '', caption)\n",
    "            # removing additional spaces\n",
    "            caption = re.sub(' +',' ',caption)\n",
    "            # add start and end tags to the caption\n",
    "            caption = '<startseq> ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' <endseq>'\n",
    "            captions[i] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69164c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the text\n",
    "clean(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e25a8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<startseq> little girl covered in paint sits in front of painted rainbow with her hands in bowl <endseq>',\n",
       " '<startseq> little girl is sitting in front of large painted rainbow <endseq>',\n",
       " '<startseq> small girl in the grass plays with fingerpaints in front of white canvas with rainbow on it <endseq>',\n",
       " '<startseq> there is girl with pigtails sitting in front of rainbow painting <endseq>',\n",
       " '<startseq> young girl with pigtails painting outside in the grass <endseq>']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping['1002674143_1b742ab4b8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c72ee75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions=[]\n",
    "for key in mapping:\n",
    "    for caption in mapping[key]:\n",
    "        all_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb2e6fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<startseq> child in pink dress is climbing up set of stairs in an entry way <endseq>',\n",
       " '<startseq> girl going into wooden building <endseq>',\n",
       " '<startseq> little girl climbing into wooden playhouse <endseq>',\n",
       " '<startseq> little girl climbing the stairs to her playhouse <endseq>',\n",
       " '<startseq> little girl in pink dress going into wooden cabin <endseq>',\n",
       " '<startseq> black dog and spotted dog are fighting <endseq>',\n",
       " '<startseq> black dog and tricolored dog playing with each other on the road <endseq>',\n",
       " '<startseq> black dog and white dog with brown spots are staring at each other in the street <endseq>',\n",
       " '<startseq> two dogs of different breeds looking at each other on the road <endseq>',\n",
       " '<startseq> two dogs on pavement moving toward each other <endseq>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "532b118b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8797\n"
     ]
    }
   ],
   "source": [
    "#Tokenize the text\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size=len(tokenizer.word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adc4cbb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get maximum length of the caption available\n",
    "max_length=max(len(caption.split()) for caption in all_captions)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "812b7e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "image_ids=list(mapping.keys())\n",
    "split=int(len(image_ids)*0.90)\n",
    "split\n",
    "train=image_ids[:split]\n",
    "test=image_ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dd13ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Generator\n",
    "def data_generator(data_keys,mapping,features,tokenizer,max_length,vocab_size,batch_size):\n",
    "    X1,X2,y=list(),list(),list()\n",
    "    n=0\n",
    "    while 1:\n",
    "        for key in data_keys:\n",
    "            n=n+1;\n",
    "            captions=mapping[key]\n",
    "            for caption in captions:\n",
    "                seq=tokenizer.texts_to_sequences([caption])[0]\n",
    "                #split the sequence in x,y pairs\n",
    "                # x is the input and y is the output\n",
    "                for i in range(1,len(seq)):\n",
    "                    #split into input and output pairs\n",
    "                    in_seq,out_seq=seq[:i],seq[i]\n",
    "                    #pad input sequence\n",
    "                    in_seq=pad_sequences([in_seq],maxlen=max_length)[0]\n",
    "                    #encode output sequence\n",
    "                    out_seq=to_categorical([out_seq],num_classes=vocab_size)[0]\n",
    "                    \n",
    "                    #store the sequences\n",
    "                    X1.append(features[key][0])\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            if n==batch_size:\n",
    "                X1,X2,y=np.array(X1),np.array(X2),np.array(y)\n",
    "                yield [X1,X2],y\n",
    "                X1,X2,y=list(),list(),list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7189c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aeedc3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder model\n",
    "#Image feature layers\n",
    "inputs1=Input(shape=(4096,))\n",
    "fe1=Dropout(0.4)(inputs1)\n",
    "fe2=Dense(256,activation='relu')(fe1)\n",
    "#sequence feature layers\n",
    "inputs2=Input(shape=(max_length,))\n",
    "se1=Embedding(vocab_size,256,mask_zero=True)(inputs2)\n",
    "se2=Dropout(0.4)(se1)\n",
    "se3=LSTM(256)(se2)\n",
    "\n",
    "#Decoder model\n",
    "decoder1=add([fe2,se3])\n",
    "decoder2=Dense(256,activation='relu')(decoder1)\n",
    "outputs=Dense(vocab_size,activation='softmax')(decoder2)\n",
    "model=Model(inputs=[inputs1,inputs2],outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b76da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 699s 6s/step - loss: 5.6240 - accuracy: 0.1185\n",
      "100/113 [=========================>....] - ETA: 1:30 - loss: 4.5343 - accuracy: 0.1970"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "epochs=15\n",
    "batch_size=64\n",
    "steps=len(train)//batch_size\n",
    "#it means it will do  the back propagation and fetch the next data\n",
    "for i in range(epochs):\n",
    "    #create data generator\n",
    "    generator=data_generator(train,mapping,features,tokenizer,max_length,vocab_size,batch_size)\n",
    "    #fit for one epoch\n",
    "    model.fit(generator,epochs=1,steps_per_epoch=steps,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d70d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d100dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Captions for the image\n",
    "def idx_to_word(integer,tokenizer):\n",
    "    for word,index in tokenizer.word_index.items():\n",
    "        if index==integer:\n",
    "            return word\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5980b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate caption for an image\n",
    "def predict_caption(model,image,tokenizer,max_length):\n",
    "    #add start tag for generation process\n",
    "    in_text='<startseq>'\n",
    "    #iterate over the max length of sequence\n",
    "    for i in range(max_length):\n",
    "        #encode the input sequence\n",
    "        sequence=tokenizer.texts_to_sequences([in_text])[0]\n",
    "        #pad the sequences\n",
    "        sequence=pad_sequences([sequence],max_length)\n",
    "        #predict the next word\n",
    "        yhat=model.predict([image,sequence],verbose=0)\n",
    "        #get index with highest probablity\n",
    "        yhat=np.argmax(yhat)\n",
    "        #convert index to word\n",
    "        word=idx_to_word(yhat,tokenizer)\n",
    "        #stop if word not found\n",
    "        if word is None:\n",
    "            break\n",
    "        #append word as input for generating next word\n",
    "        in_text+=\" \"+word\n",
    "        #stop if we reach end tag\n",
    "        if word=='<endseq>':\n",
    "            break\n",
    "    return in_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acde28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "#validation\n",
    "actual,predicted=list(),list()\n",
    "for key in tqdm(test):\n",
    "    #get actual captions\n",
    "    captions=mapping[key]\n",
    "    #predict the caption for image \n",
    "    y_pred=predict_caption(model,feature[key],tokenizer,max_length)\n",
    "    #split into word\n",
    "    actual_captions=[captions.split() for caption in captions]\n",
    "    y_pred=y_pred.split()\n",
    "    #append to the list\n",
    "    actual.append(actual_captions)\n",
    "    predicted.append(y_pred)\n",
    "#calculate BLEU score\n",
    "print(\"BLEU-1 %f\" % corpus_bleu(actual,predicted,weights=(1.0,0,0,0,0)))\n",
    "print(\"BLEU-2 %f\" % corpus_bleu(actual,predicted,weights=(0.5,0.5,0,0,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6126985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the image \n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "def generate_caption(image_name):\n",
    "    image_id=image_name.split('.')\n",
    "    img_path=os.path.join(\"Images\",image_name)\n",
    "    image=Image.open(img_path)\n",
    "    captions=mapping[image_id]\n",
    "    print(\"ACTUAL\")\n",
    "    for caption in captions:\n",
    "        print(caption)\n",
    "    #predict the caption\n",
    "    y_pred=predict_caption(model,feature[image_id],tokenizer,max_length)\n",
    "    print(\"PREDICTED\")\n",
    "    print(y_pred)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f330237",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(\"1001773457_577c3a7d70.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8518d083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ddac12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
